{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation 1\n",
      "Best novelty: 0.4353, Loss Random: 5.7002, Loss Learned: 0.0516\n",
      "Generation 2\n",
      "Best novelty: 0.3474, Loss Random: 5.6961, Loss Learned: 0.0380\n",
      "Generation 3\n",
      "Best novelty: 0.3057, Loss Random: 5.6820, Loss Learned: 0.0315\n",
      "Generation 4\n",
      "Best novelty: 0.2791, Loss Random: 5.6512, Loss Learned: 0.0264\n",
      "Generation 5\n",
      "Best novelty: 0.2597, Loss Random: 5.6207, Loss Learned: 0.0210\n",
      "Generation 6\n",
      "Best novelty: 0.2386, Loss Random: 5.5468, Loss Learned: 0.0159\n",
      "Generation 7\n",
      "Best novelty: 0.2274, Loss Random: 5.4794, Loss Learned: 0.0137\n",
      "Generation 8\n",
      "Best novelty: 0.2132, Loss Random: 5.5251, Loss Learned: 0.0126\n",
      "Generation 9\n",
      "Best novelty: 0.2151, Loss Random: 5.4893, Loss Learned: 0.0129\n",
      "Generation 10\n",
      "Best novelty: 0.2161, Loss Random: 5.5070, Loss Learned: 0.0145\n",
      "Generation 11\n",
      "Best novelty: 0.2323, Loss Random: 5.5838, Loss Learned: 0.0164\n",
      "Generation 12\n",
      "Best novelty: 0.2607, Loss Random: 5.5786, Loss Learned: 0.0162\n",
      "Generation 13\n",
      "Best novelty: 0.2718, Loss Random: 5.5603, Loss Learned: 0.0159\n",
      "Generation 14\n",
      "Best novelty: 0.2986, Loss Random: 5.4570, Loss Learned: 0.0189\n",
      "Generation 15\n",
      "Best novelty: 0.3190, Loss Random: 5.3564, Loss Learned: 0.0276\n",
      "Generation 16\n",
      "Best novelty: 0.3238, Loss Random: 5.3386, Loss Learned: 0.0294\n",
      "Generation 17\n",
      "Best novelty: 0.3185, Loss Random: 5.3587, Loss Learned: 0.0259\n",
      "Generation 18\n",
      "Best novelty: 0.3030, Loss Random: 5.3393, Loss Learned: 0.0215\n",
      "Generation 19\n",
      "Best novelty: 0.2883, Loss Random: 5.3596, Loss Learned: 0.0193\n",
      "Generation 20\n",
      "Best novelty: 0.2650, Loss Random: 5.4536, Loss Learned: 0.0209\n",
      "Generation 21\n",
      "Best novelty: 0.3327, Loss Random: 5.5662, Loss Learned: 0.0252\n",
      "Generation 22\n",
      "Best novelty: 0.3753, Loss Random: 5.4698, Loss Learned: 0.0301\n",
      "Generation 23\n",
      "Best novelty: 0.4116, Loss Random: 5.5243, Loss Learned: 0.0359\n",
      "Generation 24\n",
      "Best novelty: 0.4466, Loss Random: 5.4790, Loss Learned: 0.0413\n",
      "Generation 25\n",
      "Best novelty: 0.4799, Loss Random: 5.4724, Loss Learned: 0.0403\n",
      "Generation 26\n",
      "Best novelty: 0.4737, Loss Random: 5.5479, Loss Learned: 0.0420\n",
      "Generation 27\n",
      "Best novelty: 0.4596, Loss Random: 5.5210, Loss Learned: 0.0405\n",
      "Generation 28\n",
      "Best novelty: 0.4573, Loss Random: 5.5057, Loss Learned: 0.0350\n",
      "Generation 29\n",
      "Best novelty: 0.4373, Loss Random: 5.5676, Loss Learned: 0.0329\n",
      "Generation 30\n",
      "Best novelty: 0.4290, Loss Random: 5.5353, Loss Learned: 0.0292\n",
      "Generation 31\n",
      "Best novelty: 0.3567, Loss Random: 5.5077, Loss Learned: 0.0295\n",
      "Generation 32\n",
      "Best novelty: 0.3441, Loss Random: 5.4864, Loss Learned: 0.0299\n",
      "Generation 33\n",
      "Best novelty: 0.3456, Loss Random: 5.4254, Loss Learned: 0.0283\n",
      "Generation 34\n",
      "Best novelty: 0.3491, Loss Random: 5.4348, Loss Learned: 0.0286\n",
      "Generation 35\n",
      "Best novelty: 0.3519, Loss Random: 5.4101, Loss Learned: 0.0302\n",
      "Generation 36\n",
      "Best novelty: 0.3450, Loss Random: 5.4263, Loss Learned: 0.0303\n",
      "Generation 37\n",
      "Best novelty: 0.3146, Loss Random: 5.5215, Loss Learned: 0.0262\n",
      "Generation 38\n",
      "Best novelty: 0.3065, Loss Random: 5.5065, Loss Learned: 0.0245\n",
      "Generation 39\n",
      "Best novelty: 0.2991, Loss Random: 5.5410, Loss Learned: 0.0240\n",
      "Generation 40\n",
      "Best novelty: 0.3023, Loss Random: 5.5753, Loss Learned: 0.0245\n",
      "Generation 41\n",
      "Best novelty: 0.2930, Loss Random: 5.5323, Loss Learned: 0.0221\n",
      "Generation 42\n",
      "Best novelty: 0.2842, Loss Random: 5.5666, Loss Learned: 0.0177\n",
      "Generation 43\n",
      "Best novelty: 0.2652, Loss Random: 5.5257, Loss Learned: 0.0153\n",
      "Generation 44\n",
      "Best novelty: 0.2706, Loss Random: 5.5160, Loss Learned: 0.0148\n",
      "Generation 45\n",
      "Best novelty: 0.2421, Loss Random: 5.5131, Loss Learned: 0.0114\n",
      "Generation 46\n",
      "Best novelty: 0.2517, Loss Random: 5.5581, Loss Learned: 0.0102\n",
      "Generation 47\n",
      "Best novelty: 0.2100, Loss Random: 5.5295, Loss Learned: 0.0063\n",
      "Generation 48\n",
      "Best novelty: 0.1882, Loss Random: 5.5503, Loss Learned: 0.0038\n",
      "Generation 49\n",
      "Best novelty: 0.1441, Loss Random: 5.5890, Loss Learned: 0.0026\n",
      "Generation 50\n",
      "Best novelty: 0.0998, Loss Random: 5.5576, Loss Learned: 0.0028\n",
      "Generation 51\n",
      "Best novelty: 0.1145, Loss Random: 5.5744, Loss Learned: 0.0029\n",
      "Generation 52\n",
      "Best novelty: 0.1237, Loss Random: 5.5709, Loss Learned: 0.0037\n",
      "Generation 53\n",
      "Best novelty: 0.1545, Loss Random: 5.5328, Loss Learned: 0.0049\n",
      "Generation 54\n",
      "Best novelty: 0.1474, Loss Random: 5.5824, Loss Learned: 0.0048\n",
      "Generation 55\n",
      "Best novelty: 0.1536, Loss Random: 5.5278, Loss Learned: 0.0041\n",
      "Generation 56\n",
      "Best novelty: 0.1657, Loss Random: 5.5675, Loss Learned: 0.0043\n",
      "Generation 57\n",
      "Best novelty: 0.1403, Loss Random: 5.5332, Loss Learned: 0.0028\n",
      "Generation 58\n",
      "Best novelty: 0.1197, Loss Random: 5.4893, Loss Learned: 0.0022\n",
      "Generation 59\n",
      "Best novelty: 0.1054, Loss Random: 5.4884, Loss Learned: 0.0012\n",
      "Generation 60\n",
      "Best novelty: 0.1028, Loss Random: 5.5103, Loss Learned: 0.0006\n",
      "Generation 61\n",
      "Best novelty: 0.1063, Loss Random: 5.5652, Loss Learned: 0.0009\n",
      "Generation 62\n",
      "Best novelty: 0.0639, Loss Random: 5.5774, Loss Learned: 0.0005\n",
      "Generation 63\n",
      "Best novelty: 0.0646, Loss Random: 5.5892, Loss Learned: 0.0009\n",
      "Generation 64\n",
      "Best novelty: 0.0671, Loss Random: 5.5040, Loss Learned: 0.0012\n",
      "Generation 65\n",
      "Best novelty: 0.0699, Loss Random: 5.5687, Loss Learned: 0.0010\n",
      "Generation 66\n",
      "Best novelty: 0.1016, Loss Random: 5.5226, Loss Learned: 0.0017\n",
      "Generation 67\n",
      "Best novelty: 0.0813, Loss Random: 5.5399, Loss Learned: 0.0013\n",
      "Generation 68\n",
      "Best novelty: 0.0721, Loss Random: 5.4892, Loss Learned: 0.0014\n",
      "Generation 69\n",
      "Best novelty: 0.0777, Loss Random: 5.5755, Loss Learned: 0.0011\n",
      "Generation 70\n",
      "Best novelty: 0.0710, Loss Random: 5.5607, Loss Learned: 0.0009\n",
      "Generation 71\n",
      "Best novelty: 0.0776, Loss Random: 5.6151, Loss Learned: 0.0009\n",
      "Generation 72\n",
      "Best novelty: 0.0743, Loss Random: 5.5245, Loss Learned: 0.0007\n",
      "Generation 73\n",
      "Best novelty: 0.0656, Loss Random: 5.6203, Loss Learned: 0.0006\n",
      "Generation 74\n",
      "Best novelty: 0.0549, Loss Random: 5.6424, Loss Learned: 0.0005\n",
      "Generation 75\n",
      "Best novelty: 0.0557, Loss Random: 5.5302, Loss Learned: 0.0005\n",
      "Generation 76\n",
      "Best novelty: 0.0482, Loss Random: 5.5341, Loss Learned: 0.0005\n",
      "Generation 77\n",
      "Best novelty: 0.0709, Loss Random: 5.5712, Loss Learned: 0.0006\n",
      "Generation 78\n",
      "Best novelty: 0.0827, Loss Random: 5.4769, Loss Learned: 0.0008\n",
      "Generation 79\n",
      "Best novelty: 0.0999, Loss Random: 5.5128, Loss Learned: 0.0016\n",
      "Generation 80\n",
      "Best novelty: 0.0831, Loss Random: 5.5622, Loss Learned: 0.0012\n",
      "Generation 81\n",
      "Best novelty: 0.0911, Loss Random: 5.5252, Loss Learned: 0.0015\n",
      "Generation 82\n",
      "Best novelty: 0.0766, Loss Random: 5.5514, Loss Learned: 0.0011\n",
      "Generation 83\n",
      "Best novelty: 0.0963, Loss Random: 5.5127, Loss Learned: 0.0010\n",
      "Generation 84\n",
      "Best novelty: 0.1121, Loss Random: 5.4891, Loss Learned: 0.0009\n",
      "Generation 85\n",
      "Best novelty: 0.0622, Loss Random: 5.5879, Loss Learned: 0.0006\n",
      "Generation 86\n",
      "Best novelty: 0.0695, Loss Random: 5.5759, Loss Learned: 0.0007\n",
      "Generation 87\n",
      "Best novelty: 0.0579, Loss Random: 5.5814, Loss Learned: 0.0005\n",
      "Generation 88\n",
      "Best novelty: 0.0747, Loss Random: 5.5203, Loss Learned: 0.0005\n",
      "Generation 89\n",
      "Best novelty: 0.0829, Loss Random: 5.5043, Loss Learned: 0.0005\n",
      "Generation 90\n",
      "Best novelty: 0.0927, Loss Random: 5.5432, Loss Learned: 0.0007\n",
      "Generation 91\n",
      "Best novelty: 0.0954, Loss Random: 5.5446, Loss Learned: 0.0007\n",
      "Generation 92\n",
      "Best novelty: 0.0922, Loss Random: 5.5157, Loss Learned: 0.0008\n",
      "Generation 93\n",
      "Best novelty: 0.0935, Loss Random: 5.5178, Loss Learned: 0.0009\n",
      "Generation 94\n",
      "Best novelty: 0.0884, Loss Random: 5.6135, Loss Learned: 0.0012\n",
      "Generation 95\n",
      "Best novelty: 0.0909, Loss Random: 5.5688, Loss Learned: 0.0009\n",
      "Generation 96\n",
      "Best novelty: 0.0864, Loss Random: 5.5499, Loss Learned: 0.0008\n",
      "Generation 97\n",
      "Best novelty: 0.0771, Loss Random: 5.5369, Loss Learned: 0.0008\n",
      "Generation 98\n",
      "Best novelty: 0.0886, Loss Random: 5.5655, Loss Learned: 0.0007\n",
      "Generation 99\n",
      "Best novelty: 0.0821, Loss Random: 5.5830, Loss Learned: 0.0005\n",
      "Generation 100\n",
      "Best novelty: 0.0879, Loss Random: 5.4524, Loss Learned: 0.0004\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "\n",
    "# エンコーダの定義\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim, num_layers=3):\n",
    "        super(Encoder, self).__init__()\n",
    "\n",
    "        self.layers = nn.ModuleList()\n",
    "        self.layers.append(nn.Linear(input_dim, hidden_dim[0]))\n",
    "        for i in range(num_layers - 2):\n",
    "            self.layers.append(nn.Linear(hidden_dim[i], hidden_dim[i + 1]))\n",
    "        self.layers.append(nn.Linear(hidden_dim[-1], output_dim))\n",
    "\n",
    "    # def forward(self, x):\n",
    "    #     for layer in self.layers[:-1]:\n",
    "    #         x = torch.relu(layer(x))\n",
    "    #     x = self.layers[-1](x)\n",
    "    #     return x\n",
    "\n",
    "    def forward(self, x):\n",
    "        for layer in self.layers:\n",
    "            x = torch.relu(layer(x))\n",
    "        return x\n",
    "\n",
    "# データ拡張（ノイズの追加）\n",
    "def data_augmentation(x):\n",
    "    noise = torch.randn_like(x) * 0.05  # ノイズの強度は調整可能\n",
    "    return x + noise\n",
    "\n",
    "# 対照損失関数（NT-Xent Loss）\n",
    "def nt_xent_loss(z_i, z_j, temperature=0.5):\n",
    "    z_i = F.normalize(z_i, dim=1)\n",
    "    z_j = F.normalize(z_j, dim=1)\n",
    "    representations = torch.cat([z_i, z_j], dim=0)\n",
    "    similarity_matrix = torch.matmul(representations, representations.T) / temperature\n",
    "\n",
    "    # ラベルの作成\n",
    "    batch_size = z_i.size(0)\n",
    "    labels = torch.arange(batch_size).to(z_i.device)\n",
    "    labels = torch.cat([labels, labels], dim=0)\n",
    "\n",
    "    # 対角成分を無視するマスク\n",
    "    mask = ~torch.eye(2 * batch_size, dtype=torch.bool).to(z_i.device)\n",
    "    similarity_matrix = similarity_matrix.masked_select(mask).view(2 * batch_size, -1)\n",
    "\n",
    "    loss = F.cross_entropy(similarity_matrix, labels)\n",
    "    return loss\n",
    "\n",
    "# 行動の評価（例:環境でのシミュレーション）\n",
    "def evaluate_behavior(agent):\n",
    "    # ここでは簡単な2次元行動記述子を仮定し、ランダムに生成\n",
    "    behavior_descriptor = np.random.rand(2)  # 2次元の行動記述子\n",
    "    fitness = np.random.rand()  # 適応度も仮定\n",
    "    return behavior_descriptor, fitness\n",
    "\n",
    "# 新奇性の計算\n",
    "def calculate_novelty(behavior, random_encoder, learned_encoder):\n",
    "    with torch.no_grad():\n",
    "        behavior_tensor = torch.tensor(behavior, dtype=torch.float32).unsqueeze(0)\n",
    "        rand_embedding = random_encoder(behavior_tensor)\n",
    "        learned_embedding = learned_encoder(behavior_tensor)\n",
    "    novelty = torch.norm(rand_embedding - learned_embedding, p=2).item()  # ユークリッド距離\n",
    "    return novelty\n",
    "\n",
    "# エージェントのクラス\n",
    "class Agent:\n",
    "    def __init__(self):\n",
    "        self.behavior = None\n",
    "        self.fitness = 0\n",
    "        self.novelty = 0\n",
    "\n",
    "# 世代の進化\n",
    "def evolve_population(population, random_encoder, learned_encoder, generations=50, population_size=100, offspring_size=50, novelty_threshold=1.0, learning_rate=1e-2):\n",
    "    optimizer_random = optim.Adam(random_encoder.parameters(), lr=learning_rate)\n",
    "    optimizer_learned = optim.Adam(learned_encoder.parameters(), lr=learning_rate)\n",
    "\n",
    "    for generation in range(generations):\n",
    "        print(f\"Generation {generation + 1}\")\n",
    "\n",
    "        # 各個体の行動と新奇性を評価\n",
    "        for agent in population:\n",
    "            agent.behavior, agent.fitness = evaluate_behavior(agent)\n",
    "            agent.novelty = calculate_novelty(agent.behavior, random_encoder, learned_encoder)\n",
    "\n",
    "        # 新奇性に基づく選択（新奇性が高い順に選択）\n",
    "        population = sorted(population, key=lambda x: x.novelty, reverse=True)\n",
    "        survivors = population[:population_size]  # 上位個体を次世代に選択\n",
    "\n",
    "        # 新奇な個体を追加\n",
    "        offsprings = [Agent() for _ in range(offspring_size)]\n",
    "        for offspring in offsprings:\n",
    "            offspring.behavior, offspring.fitness = evaluate_behavior(offspring)\n",
    "            offspring.novelty = calculate_novelty(offspring.behavior, random_encoder, learned_encoder)\n",
    "\n",
    "        population = survivors + offsprings\n",
    "\n",
    "        # エンコーダを訓練\n",
    "        behaviors = [agent.behavior for agent in population]\n",
    "        behaviors = torch.tensor(behaviors, dtype=torch.float32)\n",
    "\n",
    "        # データ拡張（2つのビューを生成）\n",
    "        x_i = data_augmentation(behaviors)\n",
    "        x_j = data_augmentation(behaviors)\n",
    "\n",
    "        # ランダムエンコーダの訓練（対照学習）\n",
    "        optimizer_random.zero_grad()\n",
    "        z_i = random_encoder(x_i)\n",
    "        z_j = random_encoder(x_j)\n",
    "        loss_random = nt_xent_loss(z_i, z_j)\n",
    "        loss_random.backward()\n",
    "        optimizer_random.step()\n",
    "\n",
    "        # 学習エンコーダの訓練（MSEロスを維持）\n",
    "        optimizer_learned.zero_grad()\n",
    "        rand_embeddings = random_encoder(behaviors)\n",
    "        learned_embeddings = learned_encoder(behaviors)\n",
    "        loss_learned = torch.nn.functional.mse_loss(learned_embeddings, rand_embeddings)\n",
    "        loss_learned.backward()\n",
    "        optimizer_learned.step()\n",
    "\n",
    "        print(f\"Best novelty: {population[0].novelty:.4f}, Loss Random: {loss_random.item():.4f}, Loss Learned: {loss_learned.item():.4f}\")\n",
    "\n",
    "    return population\n",
    "\n",
    "# メイン\n",
    "if __name__ == \"__main__\":\n",
    "    # 行動空間次元、隠れ層次元、エンコーダ出力次元の設定\n",
    "    input_dim = 2  # 行動記述子の次元（ここでは2次元）\n",
    "    hidden_dim_rand = [16, 16]\n",
    "    hidden_dim_learned = [10, 12, 14, 16]\n",
    "    output_dim = 2  # 埋め込み空間の次元\n",
    "\n",
    "    # ランダムエンコーダと学習エンコーダを初期化\n",
    "    random_encoder = Encoder(input_dim, hidden_dim_rand, output_dim, num_layers=3)\n",
    "    learned_encoder = Encoder(input_dim, hidden_dim_learned, output_dim, num_layers=5)\n",
    "\n",
    "    # ランダムエンコーダのパラメータを学習可能に設定（変更点）\n",
    "\n",
    "    # 初期個体群の作成\n",
    "    population_size = 100\n",
    "    population = [Agent() for _ in range(population_size)]\n",
    "\n",
    "    # BR-NSアルゴリズムの実行\n",
    "    evolved_population = evolve_population(\n",
    "        population, random_encoder, learned_encoder,\n",
    "        generations=100, population_size=100, offspring_size=50\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env_neats",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
